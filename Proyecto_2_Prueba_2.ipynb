{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d301bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3fba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b302e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e4447e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2003</td>\n",
       "      <td>Most</td>\n",
       "      <td>most is the story of a single father who takes...</td>\n",
       "      <td>['Short', 'Drama']</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>2008</td>\n",
       "      <td>How to Be a Serial Killer</td>\n",
       "      <td>a serial killer decides to teach the secrets o...</td>\n",
       "      <td>['Comedy', 'Crime', 'Horror']</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>1941</td>\n",
       "      <td>A Woman's Face</td>\n",
       "      <td>in sweden ,  a female blackmailer with a disfi...</td>\n",
       "      <td>['Drama', 'Film-Noir', 'Thriller']</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>1954</td>\n",
       "      <td>Executive Suite</td>\n",
       "      <td>in a friday afternoon in new york ,  the presi...</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>1990</td>\n",
       "      <td>Narrow Margin</td>\n",
       "      <td>in los angeles ,  the editor of a publishing h...</td>\n",
       "      <td>['Action', 'Crime', 'Thriller']</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                      title  \\\n",
       "3107  2003                       Most   \n",
       "900   2008  How to Be a Serial Killer   \n",
       "6724  1941             A Woman's Face   \n",
       "4704  1954            Executive Suite   \n",
       "2582  1990              Narrow Margin   \n",
       "\n",
       "                                                   plot  \\\n",
       "3107  most is the story of a single father who takes...   \n",
       "900   a serial killer decides to teach the secrets o...   \n",
       "6724  in sweden ,  a female blackmailer with a disfi...   \n",
       "4704  in a friday afternoon in new york ,  the presi...   \n",
       "2582  in los angeles ,  the editor of a publishing h...   \n",
       "\n",
       "                                  genres  rating  \n",
       "3107                  ['Short', 'Drama']     8.0  \n",
       "900        ['Comedy', 'Crime', 'Horror']     5.6  \n",
       "6724  ['Drama', 'Film-Noir', 'Thriller']     7.2  \n",
       "4704                           ['Drama']     7.4  \n",
       "2582     ['Action', 'Crime', 'Thriller']     6.6  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematizacion \n",
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline\n",
    "\n",
    "# Importación de librerias\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Definición de la función que tenga como parámetro texto y devuelva una lista de lemas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Creación de matrices de documentos usando CountVectorizer, usando el parámetro 'split_into_lemmas'\n",
    "vect_lemas = CountVectorizer(stop_words='english', analyzer=split_into_lemmas)\n",
    "\n",
    "X_dtm = vect_lemas.fit_transform(dataTraining['plot'])\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b89e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de variables predictoras (X)\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f24e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigramas \n",
    "# Definición de variables predictoras (X)\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d776bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7895, 450140)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Idf \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False) ##Pruebas 2\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=(1, 2)) # puse bigramas\n",
    "X_dtm_1 = vect.fit_transform(dataTraining['plot'])\n",
    "\n",
    "X_dtm = tfidf_transformer.fit_transform(X_dtm_1)\n",
    "X_dtm.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c08c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de variable de interés (y)\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41642de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usandola función train_test_split\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Definición y entrenamiento\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=100, max_depth=30, random_state=42))\n",
    "clf.fit(X_train, y_train_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c83dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción del modelo de clasificación\n",
    "y_pred_genres = clf.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')\n",
    "\n",
    "# Sin lemas 0.7963860628253788\n",
    "# COn lemas 0.7983106666028829\n",
    "# COn bigramas 0.7737576554408515\n",
    "# Con IDF 0.807300692490283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b991d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformación variables predictoras X del conjunto de test\n",
    "X_test_dtm = vect.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "# Predicción del conjunto de test\n",
    "y_pred_test_genres = clf.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c84a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Definición y entrenamiento\n",
    "#lr = OneVsRestClassifier(LogisticRegression(random_state=123, solver='lbfgs', max_iter=3000, C=0.01, n_jobs=-1))\n",
    "# lr = OneVsRestClassifier(LogisticRegression(random_state=123, solver='newton-cg', max_iter=500, C=0.1, n_jobs=-1, penalty='l2')) --- el mejor score \n",
    "lr = OneVsRestClassifier(LogisticRegression(random_state=123, solver='saga', max_iter=500, C=1, n_jobs=-1)) \n",
    "lr.fit(X_train, y_train_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción del modelo de clasificación\n",
    "y_pred_genres = lr.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')\n",
    "\n",
    "# Sin lematizacion 0.8591001418951785\n",
    "# Con lematizacion 0.8361467846711504\n",
    "# Sin lematizar con bigramas 0.8598116682546477\n",
    "# sin Lematizar con trigramas 0.8590851191421395\n",
    "# Con iDF 0.8710286526905197 - sag 0.8710200204640789 - newton-cg 0.8710310801528628\n",
    "# COn IDF sag 500 = 0.8721597443007804\n",
    "# COn IDF saga 500 C 1 = 0.8763304693871596\n",
    "# Con IDB y Bigramas saga = 0.8777956356836194\n",
    "# Ultimo 0.8777956356836194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Definición y entrenamiento\n",
    "lr2 = OneVsRestClassifier(LogisticRegression(random_state=123, n_jobs=-1))\n",
    "lr2.fit(X_train, y_train_genres)\n",
    "\n",
    "# Predicción del modelo de clasificación\n",
    "y_pred_genres = lr2.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = OneVsRestClassifier(DecisionTreeClassifier(random_state=123))\n",
    "dt.fit(X_train, y_train_genres)\n",
    "y_pred = dt.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08311a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = OneVsRestClassifier(SVC(random_state=123))\n",
    "svm.fit(X_train, y_train_genres)\n",
    "y_pred = svm.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = OneVsRestClassifier(XGBClassifier(random_state=123))\n",
    "xgb.fit(X_train, y_train_genres)\n",
    "y_pred = xgb.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = OneVsRestClassifier(AdaBoostClassifier(random_state=123))\n",
    "adaboost.fit(X_train, y_train_genres)\n",
    "y_pred = adaboost.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformación variables predictoras X del conjunto de test\n",
    "X_test_dtm_1 = vect.transform(dataTesting['plot'])\n",
    "X_test_dtm = tfidf_transformer.transform(X_test_dtm_1)\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "# Predicción del conjunto de test\n",
    "y_pred_test_genres = lr.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_LR_TIDF_3.csv', index_label='ID')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d3ca9",
   "metadata": {},
   "source": [
    "Ejemplo interesante sobre algo similar..\n",
    "\n",
    "https://github.com/tomkeith/Multi-label-classification-with-NLP\n",
    "\n",
    "\n",
    "Ejemplo de análisis \n",
    "\n",
    "https://www.kaggle.com/code/saisandeepjallepalli/imdb-5000-eda-and-data-visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb9d2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Suponiendo que tienes dos listas: 'plots' (las descripciones de las películas)\n",
    "# y 'genres' (las listas de géneros correspondientes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d05da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aa6e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38706 unique tokens found.\n"
     ]
    }
   ],
   "source": [
    "# Procesamiento del texto\n",
    "maxlen = 100  # longitud de las secuencias\n",
    "max_words = 10000  # número máximo de palabras a considerar\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dataTraining['plot'])\n",
    "sequences = tokenizer.texts_to_sequences(dataTraining['plot'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'{len(word_index)} unique tokens found.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "labels = multilabel_binarizer.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a92697fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 272,   60, 2974, ...,   44, 8503,  148],\n",
       "       [   0,    0,    0, ..., 1145,  550, 1890],\n",
       "       [   6, 2813,    3, ...,  114,   70, 1978],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  138,    4,  487],\n",
       "       [  17,    1,  653, ...,  157,    1,  341],\n",
       "       [   0,    0,    0, ...,    4, 4194, 2231]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30eeccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "198/198 [==============================] - 7s 29ms/step - loss: 0.3199 - acc: 0.2137 - val_loss: 0.2860 - val_acc: 0.1906\n",
      "Epoch 2/10\n",
      "198/198 [==============================] - 6s 31ms/step - loss: 0.2553 - acc: 0.2867 - val_loss: 0.2645 - val_acc: 0.2717\n",
      "Epoch 3/10\n",
      "198/198 [==============================] - 6s 30ms/step - loss: 0.1734 - acc: 0.4580 - val_loss: 0.2598 - val_acc: 0.3008\n",
      "Epoch 4/10\n",
      "198/198 [==============================] - 5s 27ms/step - loss: 0.0877 - acc: 0.5380 - val_loss: 0.2945 - val_acc: 0.2932\n",
      "Epoch 5/10\n",
      "198/198 [==============================] - 6s 29ms/step - loss: 0.0385 - acc: 0.5434 - val_loss: 0.3502 - val_acc: 0.2989\n",
      "Epoch 6/10\n",
      "198/198 [==============================] - 6s 29ms/step - loss: 0.0170 - acc: 0.5500 - val_loss: 0.4023 - val_acc: 0.2894\n",
      "Epoch 7/10\n",
      "198/198 [==============================] - 6s 29ms/step - loss: 0.0085 - acc: 0.5416 - val_loss: 0.4514 - val_acc: 0.3015\n",
      "Epoch 8/10\n",
      "198/198 [==============================] - 5s 27ms/step - loss: 0.0047 - acc: 0.5440 - val_loss: 0.4880 - val_acc: 0.2869\n",
      "Epoch 9/10\n",
      "198/198 [==============================] - 6s 28ms/step - loss: 0.0029 - acc: 0.5462 - val_loss: 0.5217 - val_acc: 0.2939\n",
      "Epoch 10/10\n",
      "198/198 [==============================] - 6s 30ms/step - loss: 0.0019 - acc: 0.5431 - val_loss: 0.5486 - val_acc: 0.2882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dividir los datos en un conjunto de entrenamiento y un conjunto de validación\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(multilabel_binarizer.classes_), activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c908e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 4s 37ms/step - loss: 0.3080 - acc: 0.2164 - val_loss: 0.2704 - val_acc: 0.2393\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 3s 36ms/step - loss: 0.2457 - acc: 0.2995 - val_loss: 0.2591 - val_acc: 0.2604\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 3s 38ms/step - loss: 0.2095 - acc: 0.3599 - val_loss: 0.2527 - val_acc: 0.2773\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 3s 37ms/step - loss: 0.1725 - acc: 0.4086 - val_loss: 0.2606 - val_acc: 0.2778\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 3s 34ms/step - loss: 0.1407 - acc: 0.4385 - val_loss: 0.2980 - val_acc: 0.2436\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 3s 33ms/step - loss: 0.1112 - acc: 0.4656 - val_loss: 0.3226 - val_acc: 0.2727\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 3s 36ms/step - loss: 0.0853 - acc: 0.4785 - val_loss: 0.3831 - val_acc: 0.2457\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 3s 35ms/step - loss: 0.0665 - acc: 0.4904 - val_loss: 0.4006 - val_acc: 0.2946\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 3s 38ms/step - loss: 0.0502 - acc: 0.4844 - val_loss: 0.4575 - val_acc: 0.3043\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 0.0382 - acc: 0.5025 - val_loss: 0.4887 - val_acc: 0.2714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d94ac5cd60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizar el texto\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(dataTraining['plot'])\n",
    "sequences = tokenizer.texts_to_sequences(dataTraining['plot'])\n",
    "x = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Transformar las etiquetas en un formato que se pueda alimentar en la red\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(dataTraining['genres'])\n",
    "y = multilabel_binarizer.transform(dataTraining['genres'])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Definir la red\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 50, input_length=200))\n",
    "model.add(Conv1D(100, 3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(0.01), loss='binary_crossentropy',  metrics=['acc'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, validation_split=0.1, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluar el modelo\n",
    "#y_pred = model.predict(x_test)\n",
    "#auc = roc_auc_score(y_test, y_pred, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd042a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 [==============================] - 804s 18s/step - loss: 0.3974 - accuracy: 0.1735 - val_loss: 0.2933 - val_accuracy: 0.1933\n",
      "Epoch 2/10\n",
      "14/44 [========>.....................] - ETA: 10:37 - loss: 0.2953 - accuracy: 0.2059"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_26964/4157394067.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Entrenar el modelo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m model.fit(x_train, y_train,\n\u001b[0m\u001b[0;32m     46\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1683\u001b[0m                         ):\n\u001b[0;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1685\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1686\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    924\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1756\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1757\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "# Cargamos los datos\n",
    "#df = pd.read_csv('movies.csv') # Reemplazar con tu archivo\n",
    "\n",
    "# Convertir las cadenas de lista en listas\n",
    "dataTraining['genres'] = dataTraining['genres'].apply(eval)\n",
    "\n",
    "# Inicializar MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Ajustar y transformar los géneros a representación binaria\n",
    "labels = mlb.fit_transform(dataTraining['genres'])\n",
    "\n",
    "# Preprocesamiento del texto\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(dataTraining['plot'])\n",
    "sequences = tokenizer.texts_to_sequences(dataTraining['plot'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Asegurar que todas las secuencias tengan la misma longitud\n",
    "data = pad_sequences(sequences, maxlen=500)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)\n",
    "\n",
    "# Construir el modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 128, input_length=500))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(labels[0]), activation='sigmoid')) # Cambiar 'softmax' por 'sigmoid'\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy',  # Cambiar 'categorical_crossentropy' por 'binary_crossentropy'\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
